{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "path1 = 'C:/Users/risha/Desktop/borriston/'\n",
    "path2 = 'C:/Users/risha/address_text/'\n",
    "\n",
    "for filename in os.listdir(path1):\n",
    "    x = filename [0:filename.rindex('.')]\n",
    "    with open (os.path.join(path2,x),\"r\") as f:\n",
    "        lines = f.read()\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "voc_size = 10000\n",
    "for words in sent:\n",
    "    onehot = [one_hot(word,voc_size) for word in words]\n",
    "    print(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "embedded_docs = pad_sequences(onehot,padding='post',maxlen = 10)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "embedding_features = 40\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(voc_size,10,input_length=10))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1,activation = 'relu'))\n",
    "model.compile(loss = 'binary_crossentropy',optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BORRISTON.', 'LABORATORIES, INC..\\nBoriston Laboratories, Inc..\\n5050 Beech Place.', 'Temple Hills, Maryland 20738 * INVOICE *.', 'Telephone: 301-899-3536.', 'Telex: 248838.', 'April 24, 1984.', 'LORILLARD, INC..\\n420 English Street.', 'Greensboro, NC 27405.', 'Attention: Or.', 'Harry Minnemeyer.', 'REFERENCE: Purchase Order No.', '342-A.', 'BLI REF.', '211588.', 'INVOICE NO.', ': 1588-1.', 'DESCRIPTION AMOUNT.', 'Range-Finding Study in Rats (Phase I & II) with A-92, $ 4,500.00.', 'A-146 and B-167 @$1,500.00/cpd.', 'REMITTANCE ADDRESS:.', 'Borriston Laboratories, Inc..\\nDYNAMAC BUILDING.', '11140 Rockville Pike.', 'Rockville, MD 20852.', 'TT00z600.', '* INVOICE *.', '‘A Subsidiary of Dynamac Intemational, Inc,.']\n"
     ]
    }
   ],
   "source": [
    "#BERT\n",
    "import re\n",
    "import os \n",
    "\n",
    "\n",
    "with open (r'C:/Users/risha/address_text/00920011','r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    non_empty_lines = [line + \".\" for line in lines if line.strip() != \"\"]\n",
    "    x = '\\n'.join(non_empty_lines)\n",
    "    sent = []\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(x)\n",
    "    print(sentences)\n",
    "    for i in range (len(sentences)): \n",
    "        words = re.sub('[^a-zA-Z0-9]',' ',sentences[i])\n",
    "        words = words.lower()\n",
    "        words = words.split()\n",
    "\n",
    "        sent.append (' '.join(words))\n",
    "    sent.append('Unit No-117-120 Seepz Sdf-v M I D C Andheri Mumbai Maharashtra 400093')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "import re\n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "with open (r'C:/Users/risha/address_text/00920011','r') as f:\n",
    "    lines = f.read()\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\risha/.cache\\torch\\sentence_transformers\\sbert.net_models_bert-base-nli-mean-tokens\\0_BERT were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-426fc2b92b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-nli-mean-tokens'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msentence_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msentence_embeddings3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "d=[]\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "for i in range (0,len(test)):    \n",
    "    sentence_embeddings = model.encode(test[i])\n",
    "    sentence_embeddings3 = model.encode(test3)\n",
    "    \n",
    "    cs = cosine_similarity(\n",
    "        sentence_embeddings3,\n",
    "        sentence_embeddings\n",
    "    ).tolist()\n",
    "\n",
    "\n",
    "    idx = cs[0].index(max(cs[0]))\n",
    "    if(i == 1):\n",
    "        print(max(cs[0]))\n",
    "        \n",
    "    if(max(cs[0])>0.56):\n",
    "        d.append(test[i][idx])\n",
    "    else:\n",
    "        continue\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "    \n",
    "test1 = [\"\"\"DATAMATICS\n",
    "Plot 58, Street No. 17, MIDC, \n",
    "Andheri (E), Mumbai 400 096, MH INDIA\"\"\"]\n",
    "    \n",
    "test2 = [\"\"\"DATAMATICS\n",
    "    KNOWLEDGE CENTRE, STREET NO. 17\n",
    "MIDC, ANDHERI (EAST), MUMBAI – 400 093,\n",
    "MH, INDIA\"\"\"]\n",
    "\n",
    "test3 = [\"\"\"Datamatics Inc. \n",
    "SUITE #400\n",
    "31572 INDUSTRIAL ROAD,\n",
    "DETROIT, MI – 48150\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import math\n",
    "from math import *\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_embeddings3 = model.encode(test3)\n",
    "sentence_embeddings = model.encode(test[0])\n",
    "\n",
    "print(sentence_embeddings.shape)\n",
    "\n",
    "\n",
    "se2 = np.trunc(sentence_embeddings3[0]*10)\n",
    "se2 = se2.astype(int)\n",
    "\n",
    "for i in range(0,len(test[0])):\n",
    "    se1 = np.trunc(sentence_embeddings[i]*10)\n",
    "    se1 =  se1.astype(int)\n",
    "    js = jaccard_score(se1,se2,average='micro')\n",
    "    print(js)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#skip\n",
    "import re \n",
    "\n",
    "with open (r'C:/Users/risha/address_text/00920011','r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    non_empty_lines = [line + \".\" for line in lines if line.strip() != \"\"]\n",
    "    x = '\\n'.join(non_empty_lines)\n",
    "    sent = []\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(x)\n",
    "    for i in range (len(sentences)): \n",
    "        words = re.sub('[^a-zA-Z0-9]',' ',sentences[i])\n",
    "        words = words.lower()\n",
    "        words = words.split()\n",
    "\n",
    "        sent.append (' '.join(words))\n",
    "    sent.append(\"\"\"Datamatics\n",
    "            Unit No-117-120 Seepz Sdf-v M I D C Andheri Mumbai Maharashtra 400093\"\"\")\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/risha/Desktop/borriston/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7e0eb6bae004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/risha/Desktop/borriston/'"
     ]
    }
   ],
   "source": [
    "#test\n",
    "import os\n",
    "\n",
    "path1 = 'C:/Users/risha/Desktop/borriston/'\n",
    "path2 = 'C:/Users/risha/address_text/'\n",
    "\n",
    "test = []\n",
    "for filename in os.listdir(path1):\n",
    "    x = filename [0:filename.rindex('.')]\n",
    "    with open (os.path.join(path2,x),\"r\") as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        \n",
    "        cols = []\n",
    "        line = ''\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if(lines[i] != ''):\n",
    "                line += lines[i] + ',\\n'\n",
    "            elif(line != ''):\n",
    "                cols.append(line)\n",
    "                line = ''\n",
    "            i+=1\n",
    "        test.append(cols)\n",
    "\n",
    "print(len(test[3]))\n",
    "for i in range (len(test)):\n",
    "    for j in range (len(test[i])-1,-1,-1):\n",
    "        if (test[i][j] == ' ,\\n'):\n",
    "            test[i].pop(j)\n",
    "\n",
    "print(len(test[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Removing Characters after Pincode\n",
    "import re \n",
    "\n",
    "addresses = []\n",
    "\n",
    "print(d[0])\n",
    "for i in range (0,len(d)):\n",
    "    \n",
    "    reg = re.compile(r'(\\d{5}\\-?\\d{0,4})')\n",
    "    match = reg.findall(d[i])\n",
    "    print(match)\n",
    "    if (match != []):\n",
    "        idx1 = d[i].find(match[0])\n",
    "        addresses.append([d[i][0:idx1 + len(match[0])]])\n",
    "    else:\n",
    "        addresses.append([d[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minkowski\n",
    "from math import*\n",
    "from decimal import Decimal\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "\n",
    "def minkowski_distance(x,y,p_value):\n",
    " \n",
    "        \"\"\" return minkowski distance between two lists \"\"\"\n",
    " \n",
    "        return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
    "           p_value)\n",
    " \n",
    "def nth_root(value, n_root):\n",
    "\n",
    "    \"\"\" returns the n_root of an value \"\"\"\n",
    "\n",
    "    root_value = 1/float(n_root)\n",
    "    return round (Decimal(value) ** Decimal(root_value),3)\n",
    "\n",
    "print(len(test))\n",
    "for i in range (0,len(test)):\n",
    "    k = []\n",
    "    sentence_embeddings = model.encode(test[i])\n",
    "    for j in range (0,len(test[i])):\n",
    "        k.append(minkowski_distance(sentence_embeddings3[0], sentence_embeddings[j], 3))\n",
    "    best_match = k.index(min(k))\n",
    "    print(i)\n",
    "    print(test[i][best_match])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "48\n",
    "RESEARCH LABORATORIES, INC.,\n",
    "* + INVOICE * * *,\n",
    "\n",
    "45\n",
    "Telephone. 301-899-3536,\n",
    "Telex: 248838,\n",
    "    \n",
    "20\n",
    "Resear eens,\n",
    "Laboratories, Inc. 9-398,\n",
    "\n",
    "18\n",
    "+e INVOICE # RE,\n",
    "March 25, 1983,\n",
    "Lorillard Inc.,\n",
    "\n",
    "6\n",
    "* INVOICE *,\n",
    "‘A Subsidiary of Dynamac International, Inc.,\n",
    "oe lt 5. an,\n",
    "\n",
    "2\n",
    "Invoice #: 1500-5,\n",
    "\n",
    "1\n",
    "Telephone: 301-899:3536,\n",
    "Telex 248838 * INVOICE *,\n",
    "\n",
    "\n",
    "7\n",
    "Borriston Laboratories, Inc,\n",
    "\"5050 Beech Place,\n",
    "\n",
    "25\n",
    "Borriston Laboratories, Inc.,\n",
    "Dynamac Building,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from decimal import Decimal\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_embeddings3 = model.encode(test3)\n",
    "\n",
    "def minkowski_distance(x,y,p_value):\n",
    " \n",
    "        \"\"\" return minkowski distance between two lists \"\"\"\n",
    " \n",
    "        return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
    "           p_value)\n",
    " \n",
    "def nth_root(value, n_root):\n",
    "\n",
    "    \"\"\" returns the n_root of an value \"\"\"\n",
    "\n",
    "    root_value = 1/float(n_root)\n",
    "    return round (Decimal(value) ** Decimal(root_value),3)\n",
    "\n",
    "def chk_pincode(k,best_idx,match):\n",
    "    \n",
    "    while (match == []):\n",
    "        k.pop(best_idx)\n",
    "        if(k.index(min(k)) >= best_idx):\n",
    "            best_idx = k.index(min(k)) + 1\n",
    "        else:\n",
    "            best_idx = k.index(min(k))\n",
    "        match = reg.findall(test[i][best_idx])\n",
    "    return best_idx\n",
    "\n",
    "for i in range (0,len(test)):\n",
    "    k = []\n",
    "    sentence_embeddings = model.encode(test[i])\n",
    "    for j in range (0,len(test[i])):\n",
    "        k.append(minkowski_distance(sentence_embeddings3[0], sentence_embeddings[j], 3))\n",
    "    best_idx = k.index(min(k))\n",
    "    val = test[i][best_idx]\n",
    "    \n",
    "    reg = re.compile(r'(\\d{5}\\-?\\d{0,4})')\n",
    "    match = reg.findall(val)\n",
    "    if (match == []):\n",
    "        best_idx = chk_pincode(k,best_idx,match)\n",
    "\n",
    "    print(i)\n",
    "    print(test[i][best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*\n",
    "from decimal import Decimal\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "\n",
    "def minkowski_distance(x,y,p_value):\n",
    " \n",
    "        \"\"\" return minkowski distance between two lists \"\"\"\n",
    " \n",
    "        return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
    "           p_value)\n",
    " \n",
    "def nth_root(value, n_root):\n",
    "\n",
    "    \"\"\" returns the n_root of an value \"\"\"\n",
    "\n",
    "    root_value = 1/float(n_root)\n",
    "    return round (Decimal(value) ** Decimal(root_value),3)\n",
    "\n",
    "print(test[1])\n",
    "k = []\n",
    "sentence_embeddings = model.encode(test[1])\n",
    "for j in range (0,len(test[1])):\n",
    "    k.append(minkowski_distance(sentence_embeddings3[0], sentence_embeddings[j], 3))\n",
    "for l in k:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "non_empty_lines = [line + \".\" for line in test[0] if line.strip() != \"\"]\n",
    "x = '\\n'.join(non_empty_lines)\n",
    "sent = []\n",
    "\n",
    "sentences = nltk.sent_tokenize(x)\n",
    "ps = PorterStemmer()\n",
    "for i in range (len(sentences)):\n",
    "\n",
    "    words = re.sub('[^a-zA-Z0-9]',' ',sentences[i])\n",
    "    words = words.lower()\n",
    "    words = words.split()\n",
    "    words = [ps.stem(word) for word in words if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    sent.append (' '.join(words))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(ngram_range=(2,2))\n",
    "X = cv.fit_transform(sent).toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "non_empty_lines = [line + \".\" for line in test3 if line.strip() != \"\"]\n",
    "x = '\\n'.join(non_empty_lines)\n",
    "\n",
    "sent = []\n",
    "\n",
    "sentences = nltk.sent_tokenize(x)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range (len(sentences)):\n",
    "    words = re.sub('[^a-zA-Z0-9]',' ',sentences[i])\n",
    "    words = words.lower()\n",
    "    words = words.split()\n",
    "    words = [ps.stem(word) for word in words if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    sent.append (' '.join(words))\n",
    "print(sent)\n",
    "\n",
    "\n",
    "cv = TfidfVectorizer(ngram_range=(2,2))\n",
    "X1 = cv.fit_transform(sent).toarray()\n",
    "print(X1.shape)\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test[0][2]\n",
    "\n",
    "sent=[]\n",
    "sentences = nltk.sent_tokenize(x)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range (len(sentences)):\n",
    "    words_t = re.sub('[^a-zA-Z0-9]',' ',sentences[i])\n",
    "    words_t = words_t.lower()\n",
    "    words_t = words_t.split()\n",
    "    words_t = [ps.stem(word) for word in words_t if not word in set(stopwords.words('english'))]\n",
    "    sent.append (' '.join(words_t))\n",
    "    \n",
    "print(sent)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv1 = TfidfVectorizer(ngram_range = (2,2))\n",
    "X2 = cv1.fit_transform(sent).toarray()\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X3 =[]\n",
    "X3.append((X2[0][:9]))\n",
    "print(X3)\n",
    "print(X1)\n",
    "cs = cosine_similarity(\n",
    "    X1 , X3\n",
    ").tolist()\n",
    "    \n",
    "print(cs)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from decimal import Decimal\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def minkowski_distance(x,y,p_value):\n",
    " \n",
    "        \"\"\" return minkowski distance between two lists \"\"\"\n",
    " \n",
    "        return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
    "           p_value)\n",
    " \n",
    "def nth_root(value, n_root):\n",
    "\n",
    "    \"\"\" returns the n_root of an value \"\"\"\n",
    "\n",
    "    root_value = 1/float(n_root)\n",
    "    return round (Decimal(value) ** Decimal(root_value),3)\n",
    "\n",
    "print(minkowski_distance(X1[0],X2[0],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer ,CountVectorizer\n",
    "\n",
    "cv=CountVectorizer() \n",
    "\n",
    "\n",
    "\n",
    "word_count_vector=cv.fit_transform(test3)\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "count_vector=cv.transform(test3)\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "\n",
    "feature_names = cv.get_feature_names()\n",
    "print(tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "for i in range (len(test_180)):\n",
    "    p = s[i]\n",
    "\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',p)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub('r\\s+',' ',text)\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    for i in range (len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "\n",
    "    model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "    words = model.wv.key_to_index\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for word2vec\n",
    "\n",
    "s = []\n",
    "for i in range (len(test_180)):\n",
    "    x = ''\n",
    "    for j in range (len(test_180[i])):\n",
    "        x += test_180[i][j]\n",
    "    s.append(x)\n",
    "print(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "for k in range (len(test)):\n",
    "    data = test[k]\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "\n",
    "    max_epochs = 100\n",
    "    vec_size = 20\n",
    "    alpha = 0.025\n",
    "\n",
    "    model = Doc2Vec(vector_size = vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=20)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    model.save(\"d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    model= Doc2Vec.load(\"d2v.model\")\n",
    "    #to find the vector of a document which is not in training data\n",
    "    test_data = word_tokenize(test3[0].lower())\n",
    "    v1 = model.infer_vector(test_data)\n",
    "    print(\"V1_infer\", v1)\n",
    "\n",
    "    # to find most similar doc using tags\n",
    "    similar_doc = model.docvecs.most_similar('1')\n",
    "    print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "\n",
    "for k in range (10):\n",
    "    data = test_180[k]\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "\n",
    "    max_epochs = 100\n",
    "    vec_size = 20\n",
    "    alpha = 0.025\n",
    "\n",
    "    model = Doc2Vec(vector_size = vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=20)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    model.save(\"d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    model= Doc2Vec.load(\"d2v.model\")\n",
    "    #to find the vector of a document which is not in training data\n",
    "    test_data = word_tokenize(test3[0].lower())\n",
    "    v1 = model.infer_vector(test_data)\n",
    "    print(\"V1_infer\", v1)\n",
    "\n",
    "    # to find most similar doc using tags\n",
    "    similar_doc = model.docvecs.most_similar('1')\n",
    "    print(similar_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_180[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_180\n",
    "import os\n",
    "import re\n",
    "\n",
    "path1 = 'C:/Users/risha/Desktop/181_test/'\n",
    "path2 = 'C:/Users/risha/address_text/'\n",
    "\n",
    "test_180 = []\n",
    "for filename in os.listdir(path1):\n",
    "    x = filename [0:filename.rindex('.')]\n",
    "    with open (os.path.join(path2,x),\"r\") as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        \n",
    "        cols = []\n",
    "        line = ''\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if(lines[i] != ''):\n",
    "                line += lines[i] + ',\\n'\n",
    "            elif(line != ''):\n",
    "                cols.append(line)\n",
    "                line = ''\n",
    "            i+=1\n",
    "        test_180.append(cols)\n",
    "\n",
    "print(len(test_180[100]))\n",
    "no_alpha = re.compile('[^a-zA-Z]')\n",
    "for i in range (len(test_180)):\n",
    "    for j in range (len(test_180[i])-1,-1,-1):\n",
    "        if (re.match(no_alpha,test_180[i][j])):\n",
    "            test_180[i].pop(j)\n",
    "\n",
    "print(test_180[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((test_180[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"C:/Users/risha/Desktop/180_non_address.txt\",'r',encoding=\"utf8\") as f:\n",
    "    lines = f.read().split('\\n\\n')\n",
    "    print(len(lines))\n",
    "    \n",
    "for i in range (len(lines)):\n",
    "    p = lines[i]\n",
    "\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',p)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub('r\\s+',' ',text)\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    for i in range (len(sentences)):\n",
    "        sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "\n",
    "    model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "    words = model.wv.key_to_index\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "with open ('invoice5000.txt', 'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    for line in lines:\n",
    "        x = line.rindex('/')\n",
    "        path = line[0:x]\n",
    "        source = line[0:line.rindex(' ')]\n",
    "        str = \"\"\n",
    "        destination = \"invoice_\" + str.join(os.listdir(path))\n",
    "        shutil.copyfile(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'C:/Users/risha/Desktop/180_text_address'\n",
    "\n",
    "for filename in os.listdir(path1):\n",
    "    with open (os.path.join(path1,filename),\"r\") as f:\n",
    "        lines = f.read()\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "with open (r'C:/Users/risha/Desktop/addresses_match.txt','r') as f:\n",
    "    lines = f.read().split('\\n\\n')\n",
    "\n",
    "for i in range (len(lines)):\n",
    "    lines[i] = re.sub('[^a-zA-Z]',' ', lines[i]) \n",
    "    lines[i] = \" \".join(lines[i].split())\n",
    "    lines[i] = lines[i].lower()\n",
    "\n",
    "print(lines)\n",
    "cv= TfidfVectorizer()\n",
    "X = cv.fit_transform(lines).toarray()\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1,1,1,1,1,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "print(X_test.shape)\n",
    "pred = classifier.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['Kasturba Main Road, Opposite Salute Restaurant, Kastur Park, Borivali, Mumbai, Maharashtra 400092']\n",
    "\n",
    "\n",
    "test[0]= re.sub('[^a-zA-Z]',' ', test[0]) \n",
    "test[0] = \" \".join(test[0].split())\n",
    "test[0] = test[0].lower()\n",
    "\n",
    "print(test)\n",
    "new = cv.fit_transform(test).toarray()\n",
    "new.resize(2,30)\n",
    "print(new.shape)\n",
    "print(new)\n",
    "\n",
    "pred = classifier.predict(new)\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:/Users/risha/OneDrive/Documents/Copy of address_50.csv',encoding = 'latin')\n",
    "df = df.dropna()\n",
    "df = df.reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('LABEL',axis=1)\n",
    "X.head()\n",
    "\n",
    "y = df['LABEL']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "text = df.copy()\n",
    "\n",
    "print(len(text))\n",
    "corpus = []\n",
    "for i in range (len(text)):\n",
    "    a = re.sub('[^a-zA-Z]',' ', text['ADDRESS'][i]) \n",
    "    a = a.lower()\n",
    "    a = a.split()\n",
    "    \n",
    "    a = [ps.stem(word) for word in a if not word in stopwords.words('english')]\n",
    "    a = ' '.join(a)\n",
    "    corpus.append(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "cv= TfidfVectorizer(ngram_range=(1,2))\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "print(X_test.shape)\n",
    "pred = classifier.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(pred)\n",
    "print(score)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(classifier, X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train['ADDRESS']\n",
    "\n",
    "print(X_train.shape)\n",
    "X_test = X_test['ADDRESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X['ADDRESS']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(pipe,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)\n",
    "    \n",
    "path1 = \"C:/Users/risha/Desktop/tocr_rishabh/\"\n",
    "for filename in os.listdir(path1):\n",
    "    x = os.path.join(path1,filename)\n",
    "\n",
    "    chk = model.predict_proba(testing)\n",
    "    h = []\n",
    "    for i in chk:\n",
    "        h.append(i[1])\n",
    "    print(testing[h.index(max(h))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range = (1,3))),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf',MultinomialNB(alpha = 0.1))\n",
    "                ])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ['A/41 SITA SADAN,OFF SAI BABA NAGAR,SKY BUILD VILLAGE,KANDIVALI(WEST)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "d = pd.read_csv(r'C:/Users/risha/OneDrive/Documents/testing_address.csv',encoding = 'latin')\n",
    "d.head()\n",
    "\n",
    "X_test = d.drop('LABEL',axis=1)\n",
    "X_test = X_test['ADDRESS']\n",
    "\n",
    "y_test = d['LABEL']\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "pipe1 = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range = (1,2))),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf',MLPClassifier(alpha=1e-05, hidden_layer_sizes=(20,), random_state=1,solver='lbfgs'))\n",
    "                ])\n",
    "\n",
    "\n",
    "pipe1.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "from sklearn.metrics import accuracy_score\n",
    "    \n",
    "pred = model.predict(testing_padded)\n",
    "print(len(pred))\n",
    "for i in range(len(pred)):\n",
    "    if pred[i]>=0.5:\n",
    "        pred[i] = 1\n",
    "    else:\n",
    "        pred[i]=0\n",
    "\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)\n",
    "\n",
    "path1 = \"C:/Users/risha/Desktop/tocr_rishabh/\"\n",
    "for filename in os.listdir(path1): \n",
    "\n",
    "    with open (os.path.join(path1,filename),encoding = 'latin') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "\n",
    "    chk = model.predict_proba(lines)\n",
    "    h = []\n",
    "    for i in chk:\n",
    "        h.append(i[1])\n",
    "    print(lines[h.index(max(h))])       \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "training_padded = pad_sequences(training_sequences,padding = 'post')\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences,padding = 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = corpus[0:500]\n",
    "X_test = corpus[500:]\n",
    "y_train = y[0:500]\n",
    "y_test = y[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(1117,64),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24,activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_padded,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(p)\n",
    "\n",
    "padded = pad_sequences(sequences,padding = 'post')\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_padded[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
