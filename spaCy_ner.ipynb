{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:/Users/risha/OneDrive/Documents/Copy of address_50.csv',encoding = 'latin')\n",
    "df = df.dropna()\n",
    "df = df.reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[df['LABEL']==1]\n",
    "X = X.drop('LABEL',axis=1)\n",
    "X = X['ADDRESS']\n",
    "print(X.shape)\n",
    "\n",
    "y = df['LABEL']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "text = df.copy()\n",
    "\n",
    "corpus = []\n",
    "for i in range (len(X)):\n",
    "    a = re.sub('[^a-zA-Z]',' ', X[i]) \n",
    "    a = a.lower()\n",
    "    a = a.split()\n",
    "    \n",
    "    a = ' '.join(a)\n",
    "    corpus.append(a)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc = []\n",
    "for sent in corpus:\n",
    "    doc.append(nlp(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[3].ents)\n",
    "\n",
    "for i in doc[3].ents:\n",
    "    print(i.text,i.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style = 'ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "training_data = []\n",
    "lines = []\n",
    "\n",
    "with open('Desktop/datamatics_internship/spacy_ner(1).json', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data['content']\n",
    "        entities = []\n",
    "        for annotation in data['annotation']:\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "\n",
    "            for label in labels:\n",
    "                entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    with open('training_data', 'wb') as fp:\n",
    "        pickle.dump(training_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "with open ('training_data', 'rb') as fp:\n",
    "    TRAIN_DATA = pickle.load(fp)\n",
    "\n",
    "LABEL = ['AREA','CITY','STATE','LOCATION']\n",
    "model = None\n",
    "\n",
    "nlp = spacy.blank('en')  # create blank Language class\n",
    "print(\"Created blank 'en' model\")\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('ner')\n",
    "\n",
    "ner = nlp.get_pipe('ner')\n",
    "\n",
    "for i in LABEL:\n",
    "    ner.add_label(i)\n",
    "\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe!='ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(10):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            for texts,annotations in batch:\n",
    "                \n",
    "                doc = nlp.make_doc(texts)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print('Losses', losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = '104, Plot No. 170, Jawahar Nagar Rd, Kakaji Nagar, No. 2, Goregaon West, Mumbai, Maharashtra 400062'\n",
    "doc = nlp(test_text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
